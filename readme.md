## GRL (Game of Reinforcement Learning/General Purpose Reinforcement Learning)

A modulated and versatile library for deep reinforcement learning, implemented in *Tensorflow2.4*.

## Instructions

If you want to know how an algorithm works, simply study `agent.py` and `nn.py` in each folder of [algo](https://github.com/xlnwel/d2rl/tree/master/algo).

If you want to run some algorithm, refer to [Get Start](#start).

There is currently no plan on writing a detailed document. But you're welcome to open an issue whenever you have any questions/come across any mistakes, and I'll answer ASAP and add comments accordingly.

## Current Implemented Algorithms/Networks

Many algorithms are simply improvements/minor modifications of their predecessors. Therefore, instead of implementing them as separate algorithms, we provide options to turn these on/off in `config.yaml`. Refer to [algo/readme.md](https://github.com/xlnwel/d2rl/blob/master/algo/readme.md) for more details.

### On Policy RL

All implementation details from OpenAI's baselines are implemented for PPO families 

- [x] DAAC
- [x] PPG
- [x] PPO (with FNN)
- [x] PPO2 (with masked LSTM)
- [x] RND

### Off Policy RL

- [x] FQF
- [x] IQN
- [x] M-DQN
- [x] M-IQN
- [x] Rainbow
- [x] Reactor
- [x] Retrace(ùùÄ)
- [x] RDQN (Recurrent DQN with masked LSTM)
- [x] SAC (w/ or w/o adaptive temperature)
- [x] SACD (SAC for discrete action space)
- [x] TAC
- [x] TBO (Transformed Bellman Operator)

### Distributed RL

*ray1.1.0* is used for distributed training. 

- [x] Ape-X
- [x] R2D2
- [x] SEED

### Model-Based RL

- [x] Dreamer

### Imitation Learning

- [x] AIRL
- [x] GAIL

### Multi-Agent Reinforcement Learning

- [x] QMIX
- [x] Multi-Agent PPO

### Networks

- [x] CBAM
- [x] Convolutional Attention
- [x] DNC (Differentiable Neural Computer)
- [x] Customized LSTM
- [x] Customized GRU
- [x] MobileNet Block
- [x] Multi-Head Attention
- [x] Randomized Network (for Generalization)
- [x] ResNet
- [x] SENet
- [x] SN (Spectral Norm)

## <a name="start"></a>Get Started

### Training

```shell
python run/train.py algo -e env
```

For available `algo`, please refer to the folder names in `/algo`. To run distributed off-policy algorithms, `algo` should be of form `{distributed_architecture}-{algorithm}`. For example, if you want to run Ape-X with DQN, replace `algo` with `apex-dqn`.

`env` follows convention `{suite}_{name}`, where `{suite}_` may be omitted when there is no corresponding suite name. Current available `suite` includes `[atari, procgen, dmc]`.

All configurations are specified in `*config.yaml` in each fold following convention `{algo}_{env_suite}_config.yaml`, where `algo` is omitted when there is no ambiguous and `env_suite` is omitted when there is no corresponding suite name. 

Examples

```shell
python run/train.py ppo -e BipedalWalker-v3     # no suite specified
python run/train.py ppo -e procgen_coinrun      # procgen suite
python run/train.py iqn -e procgen_coinrun
python run/train.py apex-iqn -e procgen_coinrun
```

By default, all the checkpoints and loggings are saved in `./logs/{env}/{algo}/{model_name}/`.

You can also make some simple changes to `*config.yaml` from the command line

```
# change learning rate to 0.0001, `lr` must appear in `*config.yaml`
python run/train.py ppo -e procgen_coinrun -kw lr=0.0001
```

### Evaluation

Evaluation is simple‚Äîyou only need to know your checkpoint directory, which is by default of form `./logs/{env}/{algo}/{model_name}/`. For example, the following code trains and evaluates `ppo` on `BipedalWalker-v3`

```shell
python run/train.py ppo -e BipedalWalker-v3       # train
python run/eval.py logs/BipedalWalker-v3/ppo      # eval
````

## Acknowledge

I'd like to especially thank @danijar for his great help with Dreamer.

## Reference Papers

Machado, Marlos C., Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling. 2018. ‚ÄúRevisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents.‚Äù IJCAI International Joint Conference on Artificial Intelligence 2018-July (2013): 5573‚Äì77.

Espeholt, Lasse, Rapha√´l Marinier, Piotr Stanczyk, Ke Wang, and Marcin Michalski. 2019. ‚ÄúSEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference,‚Äù 1‚Äì19. http://arxiv.org/abs/1910.06591.

Badia, Adri√† Puigdom√®nech, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, and Charles Blundell. 2020. ‚ÄúAgent57: Outperforming the Atari Human Benchmark.‚Äù http://arxiv.org/abs/2003.13350.

Burda, Yuri, Harrison Edwards, Amos Storkey, and Oleg Klimov. 2018. ‚ÄúExploration by Random Network Distillation,‚Äù 1‚Äì17. http://arxiv.org/abs/1810.12894.

Pardo, Fabio, Arash Tavakoli, Vitaly Levdik, and Petar Kormushev. 2018. ‚ÄúTime Limits in Reinforcement Learning.‚Äù 35th International Conference on Machine Learning, ICML 2018 9: 6443‚Äì52.

Jaderberg, Max, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Casta√±eda, Charles Beattie, et al. 2019. ‚ÄúHuman-Level Performance in 3D Multiplayer Games with Population-Based Reinforcement Learning.‚Äù Science 364 (6443): 859‚Äì65. https://doi.org/10.1126/science.aau6249.

Hafner, Danijar, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. 2019. ‚ÄúLearning Latent Dynamics for Planning from Pixels.‚Äù 36th International Conference on Machine Learning, ICML 2019 2019-June: 4528‚Äì47.

Graves, Alex, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwi≈Ñska, Sergio G√≥mez Colmenarejo, et al. 2016. ‚ÄúHybrid Computing Using a Neural Network with Dynamic External Memory.‚Äù Nature 538 (7626): 471‚Äì76. https://doi.org/10.1038/nature20101.

Kapturowski, Steven, Georg Ostrovski, John Quan, and Will Dabney. 2019. ‚ÄúRecurrent Experience Replay in Distributed Reinforcement Learning.‚Äù In ICLR, 1‚Äì19.

Horgan, Dan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado van Hasselt, and David Silver. 2018. ‚ÄúDistributed Prioritized Experience Replay.‚Äù In ICLR, 1‚Äì19. http://arxiv.org/abs/1803.00933.

Haarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. ‚ÄúSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.‚Äù 35th International Conference on Machine Learning, ICML 2018 5: 2976‚Äì89.

Munos, R√©mi, Thomas Stepleton, Anna Harutyunyan, and Marc G. Bellemare. 2016. ‚ÄúSafe and Efficient Off-Policy Reinforcement Learning.‚Äù Advances in Neural Information Processing Systems, no. Nips: 1054‚Äì62.

Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. ‚ÄúProximal Policy Optimization Algorithms.‚Äù ArXiv, 1‚Äì12.

Cobbe, Karl, Jacob Hilton, Oleg Klimov, and John Schulman. 2020. ‚ÄúPhasic Policy Gradient.‚Äù http://arxiv.org/abs/2009.04416.

Haarnoja, Tuomas, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, et al. 2018. ‚ÄúSoft Actor-Critic Algorithms and Applications.‚Äù http://arxiv.org/abs/1812.05905.

Christodoulou, Petros. 2019. ‚ÄúSoft Actor-Critic for Discrete Action Settings,‚Äù 1‚Äì7. http://arxiv.org/abs/1910.07207.

Haarnoja, Tuomas, Haoran Tang, Pieter Abbeel, and Sergey Levine. 2017. ‚ÄúReinforcement Learning with Deep Energy-Based Policies.‚Äù 34th International Conference on Machine 
Learning, ICML 2017 3: 2171‚Äì86.

Vieillard, Nino, Olivier Pietquin, and Matthieu Geist. 2020. ‚ÄúMunchausen Reinforcement Learning,‚Äù no. NeurIPS. http://arxiv.org/abs/2007.14430.

Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. ‚ÄúMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.‚Äù http://arxiv.org/abs/1704.04861.

Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang Chieh Chen. 2018. ‚ÄúMobileNetV2: Inverted Residuals and Linear Bottlenecks.‚Äù Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 4510‚Äì20. https://doi.org/10.1109/CVPR.2018.00474.

Howard, Andrew, Mark Sandler, Bo Chen, Weijun Wang, Liang Chieh Chen, Mingxing Tan, Grace Chu, et al. 2019. ‚ÄúSearching for MobileNetV3.‚Äù Proceedings of the IEEE International Conference on Computer Vision 2019-October: 1314‚Äì24. https://doi.org/10.1109/ICCV.2019.00140.

He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. ‚ÄúDeep Residual Learning for Image Recognition.‚Äù Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2016-December: 770‚Äì78. https://doi.org/10.1109/CVPR.2016.90.

He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. ‚ÄúIdentity Mappings in Deep Residual Networks.‚Äù Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 9908 LNCS: 630‚Äì45. https://doi.org/10.1007/978-3-319-46493-0_38.

Tan, Mingxing, and Quoc V. Le. 2019. ‚ÄúEfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.‚Äù 36th International Conference on Machine Learning, ICML 2019 2019-June: 10691‚Äì700.

Graves, Alex, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwi≈Ñska, Sergio G√≥mez Colmenarejo, et al. 2016. ‚ÄúHybrid Computing Using a Neural Network with Dynamic External Memory.‚Äù Nature 538 (7626): 471‚Äì76. https://doi.org/10.1038/nature20101.

Hsin, Carol. 2016. ‚ÄúImplementation and Optimization of Differentiable Neural Computers.‚Äù https://web.stanford.edu/class/cs224n/reports/2753780.pdf.

Dzmitry Bahdanau, KyungHyun Cho Yoshua Bengio. 2015. ‚ÄúNeural Machine Translation by Jointly Learning to Align and Translate.‚Äù Microbes and Infection 11 (3): 367‚Äì73. https://doi.org/10.1016/j.micinf.2008.12.015.

Luong, Minh Thang, Hieu Pham, and Christopher D. Manning. 2015. ‚ÄúEffective Approaches to Attention-Based Neural Machine Translation.‚Äù Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing, 1412‚Äì21. https://doi.org/10.18653/v1/d15-1166.

Xu, Kelvin, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S Zemel, and Yoshua Bengio. 2014. ‚ÄúShow, Attend and Tell: Neural Image Caption Generation with Visual Attention.‚Äù https://doi.org/10.1109/72.279181.

Woo, Sanghyun, Jongchan Park, Joon Young Lee, and In So Kweon. 2018. ‚ÄúCBAM: Convolutional Block Attention Module.‚Äù Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 11211 LNCS: 3‚Äì19. https://doi.org/10.1007/978-3-030-01234-2_1.

Hu, Jie, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. 2020. ‚ÄúSqueeze-and-Excitation Networks.‚Äù IEEE Transactions on Pattern Analysis and Machine Intelligence 42 (8): 2011‚Äì23. https://doi.org/10.1109/TPAMI.2019.2913372.

Espeholt, Lasse, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Boron Yotam, et al. 2018. ‚ÄúIMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures.‚Äù 35th International Conference on Machine Learning, ICML 2018 4: 2263‚Äì84.

Hafner, Danijar, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. 2020. ‚ÄúDream to Control: Learning Behaviors by Latent Imagination.‚Äù ICLR, 1‚Äì20. http://arxiv.org/abs/1912.01603.

Haarnoja, Tuomas, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, et al. 2018. ‚ÄúSoft Actor-Critic Algorithms and Applications.‚Äù http://arxiv.org/abs/1812.05905.

Engstrom, Logan, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. 2019. ‚ÄúImplementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO.‚Äù ICLR, no. January.

Lee, Kimin, Kibok Lee, Jinwoo Shin, and Honglak Lee. 2020. ‚ÄúNetwork Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning.‚Äù Iclr 2020, 1‚Äì22. http://arxiv.org/abs/1910.05396.

**Please let me know if I miss any references.**

## Reference Repositories

https://github.com/openai/baselines

https://github.com/google/dopamine

https://github.com/deepmind/dnc

https://github.com/deepmind/trfl

https://github.com/google-research/seed_rl

https://github.com/danijar/dreamer

https://github.com/microsoft/FQF

https://github.com/rwightman/pytorch-image-models

https://github.com/juntang-zhuang/Adabelief-Optimizer
